{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring OLS and WLS using NumPy and matplotlib\n",
    "Author: BjÃ¶rn Dahlgren, 2016-04-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def rnd(shape, positive=True):\n",
    "    if positive:\n",
    "        return np.abs(np.random.normal(size=shape))\n",
    "        #return np.random.random(shape)\n",
    "    else:\n",
    "        return np.random.normal(size=shape)\n",
    "        #return np.random.random(shape) - 0.5\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(N=20, rel_minerr=1e-2, abs_maxerr=2, xmin=0, xmax=10):\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    err = rnd(x.shape)*(1 - rel_minerr) + rel_minerr\n",
    "    heteroscedastic=0\n",
    "    homoscedastic=1\n",
    "    y = ((np.pi + (abs_maxerr*rnd(x.shape, False)*err)*heteroscedastic)*x + \n",
    "         np.exp(1) + (abs_maxerr*rnd(x.shape, False)*err)*homoscedastic)\n",
    "    return x, y, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, err = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(*args):\n",
    "    plt.errorbar(x, y, yerr=3*err, ls='None', marker='.')\n",
    "    for beta in args:\n",
    "        plt.plot(x, beta[0] + x*beta[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plot([np.exp(1), np.pi])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(err, '.')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x/err, y/err, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LS(x, y, w=1):  # w == 1 => OLS, w != 1 => WLS\n",
    "    \"\"\" Least squares \n",
    "    \n",
    "    References\n",
    "    -----------\n",
    "    Wikipedia & standard texts on least squares.\n",
    "    A note about R2 in WLS:\n",
    "        Willett, John B., and Judith D. Singer. \"Another cautionary note about R 2:\n",
    "        Its use in weighted least-squares regression analysis.\"\n",
    "        The American Statistician 42.3 (1988): 236-238.\n",
    "    \"\"\"\n",
    "    sqrtw = np.sqrt(w)\n",
    "    y = y * sqrtw\n",
    "    X = np.ones((x.size, 2))\n",
    "    X[:, 1] = x\n",
    "    if hasattr(sqrtw, 'ndim') and sqrtw.ndim == 1:\n",
    "        sqrtw = sqrtw.reshape((sqrtw.size, 1))\n",
    "    X *= sqrtw\n",
    "    \n",
    "    beta = np.linalg.lstsq(X, y)[0]\n",
    "    eps = X.dot(beta) - y\n",
    "    SSR = eps.T.dot(eps)  # sum of squared residuals\n",
    "    vcv = SSR/(x.size - 2)*np.linalg.inv(X.T.dot(X))\n",
    "    TSS = np.sum(np.square(y - np.mean(y)))  # total sum of squares\n",
    "    R2 = 1 - SSR/TSS\n",
    "    return beta, vcv, R2\n",
    "    XtX = X.T.dot(X)\n",
    "    return np.linalg.lstsq(XtX, X.T.dot(y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x, beta):\n",
    "    X = np.ones((x.size, 2))\n",
    "    X[:, 1] = x\n",
    "    return X.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta, vcv, R2 = LS(x, y)\n",
    "beta_w, vcv_w, R2_w = LS(x, y, err**-2)\n",
    "plot([np.exp(1), np.pi], beta, beta_w)\n",
    "print(beta, np.diag(vcv)**0.5, R2)\n",
    "print(beta_w, np.diag(vcv_w)**0.5, R2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "ols_model = sm.OLS(y, sm.add_constant(x))\n",
    "wls_model = sm.WLS(y, sm.add_constant(x), weights=err**-2)\n",
    "ols_res = ols_model.fit()\n",
    "wls_res = wls_model.fit()\n",
    "print(ols_res.params, ols_res.bse, ols_res.rsquared)\n",
    "print(wls_res.params, wls_res.bse, wls_res.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot([np.exp(1), np.pi], ols_res.params, wls_res.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_N = np.logspace(1, 3, 20000)\n",
    "R2 = np.empty((2, all_N.size))\n",
    "beta = np.empty((2, all_N.size, 2))\n",
    "cov = np.empty((2, all_N.size, 2, 2))\n",
    "for idx_N, N in enumerate(all_N):\n",
    "    x, y, err = get_data(N)\n",
    "    for idx_m, w in enumerate([1, err**-2]):\n",
    "        beta[idx_m, idx_N, :], cov[idx_m, idx_N, :, :], R2[idx_m, idx] = LS(x, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "for idx_beta, true_val in enumerate([np.exp(1), np.pi]):\n",
    "    for idx_m, lbl in enumerate(['OLS', 'WLS']):\n",
    "        ax = plt.subplot(2, 2, idx_beta+1)\n",
    "        style = dict(c='bg'[idx_m], ls='None', marker='.',)\n",
    "        plt.plot(all_N, np.abs(beta[idx_m, :, idx_beta] - true_val),\n",
    "                     #yerr=cov[idx_m, :, idx_beta, idx_beta]**0.5,\n",
    "                alpha=0.1, **style)\n",
    "        plt.plot(np.nan, np.nan, label=lbl, **style)  # avoid alpha in legend\n",
    "        \n",
    "        ax = plt.subplot(2, 2, idx_beta+3)\n",
    "        plt.plot(all_N, cov[idx_m, :, idx_beta, idx_beta],\n",
    "                     #yerr=cov[idx_m, :, idx_beta, idx_beta]**0.5,\n",
    "                alpha=0.1, **style)\n",
    "        plt.plot(np.nan, np.nan, label=lbl, **style)  # avoid alpha in legend\n",
    "        \n",
    "    ax = plt.subplot(2, 2, idx_beta+1)\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend(numpoints=1, loc='best', frameon=False)\n",
    "    ax.set_xlabel(r'$n_{obs}$', fontsize=16)\n",
    "    ax.set_ylabel(r'$| \\beta_%d - \\hat{\\beta}_%d |$' % (idx_beta, idx_beta), fontsize=16)\n",
    "    \n",
    "    ax = plt.subplot(2, 2, idx_beta+3)\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend(numpoints=1, loc='best', frameon=False)\n",
    "    ax.set_xlabel(r'$n_{obs}$', fontsize=16)\n",
    "    ax.set_ylabel(r'$var(\\hat{\\beta}_%d)$' % idx_beta, fontsize=16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
